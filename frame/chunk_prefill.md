# SARATHI: Efficient LLM Inference by Piggybacking Decodes with Chunked Prefills

https://arxiv.org/pdf/2308.16369
