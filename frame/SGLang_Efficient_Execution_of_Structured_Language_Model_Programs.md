# Abstract

Large language models (LLMs) are increasingly used for complex tasks that require multiple generation calls, advanced prompting techniques, control flow, and structured inputs/outputs. However, efficient systems are lacking for programming and executing these applications. We introduce SGLang, a system for efficient execution of complex language model programs. SGLang consists of a frontend language and a runtime. The frontend simplifies programming with primitives for generation and parallelism control. The runtime accelerates execution with novel optimizations like RadixAttention for KV cache reuse and compressed finite state machines for faster structured output decoding. Experiments show that SGLang achieves up to 6.4× higher throughput compared to state-of-the-art inference systems on various large language and multi-modal models on tasks including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, and multi-turn chat. The code is publicly available at https://github.com/sgl-project/sglang.

大型语言模型 (LLM) 越来越多地用于需要多次生成调用、高级提示技术、控制流和结构化输入/输出的复杂任务。然而，缺乏用于编程和执行这些应用程序的高效系统。我们介绍了 SGLang，一个用于高效执行复杂语言模型程序的系统。 SGLang 由前端语言和运行时组成。前端使用用于生成和并行控制的原语简化了编程。运行时通过新颖的优化来加速执行，例如用于 KV 缓存重用的 RadixAttention 和用于更快的结构化输出解码的压缩有限状态机。实验表明，与各种大型语言和多模态模型上的最先进的推理系统相比，SGLang 在代理控制、逻辑推理、小样本学习基准、JSON 解码、检索增强生成管道和多轮聊天等任务上的吞吐量提高了 6.4 倍。该代码可在 https://github.com/sgl-project/sglang 上公开获取。


# 1.Introduction

Recent increases in the capabilities of LLMs have broadened their utility, enabling them to tackle a wider range of general tasks and act as autonomous agents [35, 6, 36, 52, 46]. In such applications, LLMs engage in multi-round planning, reasoning, and interaction with external environments. This is accomplished through tool usage [41, 38], multiple input modalities [47, 2], and a wide range of prompting techniques [30], like few-shot learning [5], self-consistency [53], skeleton-of-thought [33], and tree-of-thought [56]. All of these new use cases require multiple, often dependent, LLM generation calls, showing a trend of using multi-call structures to complete complex tasks [57, 21]. The emergence of these patterns signifies a shift in our interaction with LLMs, moving from simple chatting to a more sophisticated form of programmatic usage of LLMs, which means using a program to schedule and control the generation processes of LLMs. We refer to these programs as "Language Model Programs" (LM Programs) [4, 20]. The advanced prompting techniques and agentic workflow mentioned above fall within the scope of LM programs. There are two common properties of LM programs: (1) LM programs typically contain multiple LLM calls interspersed with control flow. This is needed to complete complex tasks and improve overall quality. (2) LM programs receive structured inputs and produce structured outputs. This is needed to enable the composition of LM programs and to integrate LM programs into existing software systems.

近年来，大语言模型(LLM) 能力的提升扩大了其用途，使其能够处理更广泛的一般任务，并充当自主代理 [35, 6, 36, 52, 46]。在这样的应用中，LLM 进行多轮规划、推理和与外部环境的交互。这是通过使用工具[41, 38]、多种输入模式[47, 2]和广泛的提示技术[30]（如小样本学习[5]、自洽[53]、思维骨架[33]和思维树[56]）来实现的。所有这些新的用例都需要多个、通常相互依赖的 LLM 生成调用，显示出使用多调用结构完成复杂任务的趋势 [57, 21]。这些模式的出现意味着我们与 LLM 的互动发生了转变，从简单的聊天转变为更复杂的 LLM 程序化使用形式，这意味着使用程序来安排和控制 LLM 的生成过程。我们将这些程序称为“语言模型程序”（LM 程序）[4, 20]。上面提到的高级提示技术和代理工作流程都属于 LM 程序的范围。 LM 程序有两个共同的特性：(1) LM 程序通常包含多个与控制流交错的 LLM 调用。这是完成复杂任务和提高整体质量所需要的。 （2）LM 程序接收结构化输入并产生结构化输出。这对于实现 LM 程序的组成以及将 LM 程序集成到现有软件系统中是必需的。

Despite the widespread use of LM programs, current systems for expressing and executing them remain inefficient. We identify two primary challenges associated with the efficient use of LM programs: First, programming LM programs is tedious and difficult due to the non-deterministic nature of LLMs. Developing an LM program often requires extensive string manipulation, experimental tuning of prompts, brittle output parsing, handling multiple input modalities, and implementing parallelism mechanisms. This complexity significantly reduces the readability of even simple programs (Sec. 2).

尽管 LM 程序得到了广泛使用，但当前用于表达和执行它们的系统仍然效率低下。我们发现了与高效使用 LM 程序相关的两个主要挑战：首先，由于 LLM 的非确定性，编写 LM 程序既繁琐又困难。开发 LM 程序通常需要大量的字符串操作、提示的实验性调整、脆弱的输出解析、处理多种输入模式以及实现并行机制。这种复杂性大大降低了即使是简单程序的可读性（第 2 节）。

Secondly and importantly, executing LM programs is inefficient due to redundant computation and memory usage. State-of-the-art inference engines (e.g., vLLM [23], TGI [16], and TensorRTLLM [34]), have been optimized to reduce latency and improve throughput without direct knowledge of the workload. This makes these systems general and robust but also results in significant inefficiencies for any given workload. A prominent example is the reuse of the Key-Value (KV) cache (Sec. 3). The KV cache consists of reusable intermediate tensors that are essential for generative inference. During typical batch executions of LM programs, numerous opportunities exist to reuse the KV cache across multiple different LLM calls that share a common prefix. However, current systems lack effective mechanisms to facilitate this reuse, resulting in unnecessary computations and wasted memory. Another example is constrained decoding for structured outputs (e.g., JSON mode), where the output of LLMs is restricted to follow specific grammatical rules defined by a regular expression (Sec. 4). Under these constraints, multiple tokens can often be decoded once. However, existing systems only decode one token at a time, leading to suboptimal decoding speeds. 

其次，也是最重要的一点，由于冗余计算和内存使用，执行 LM 程序效率低下。最先进的推理引擎（例如 vLLM [23]、TGI [16] 和 TensorRTLLM [34]）已经过优化，可在不直接了解工作负载的情况下减少延迟并提高吞吐量。这使得这些系统通用且强大，但也会导致任何给定工作负载的效率显著低下。一个突出的例子是重用键值 (KV) 缓存（第 3 节）。KV 缓存由可重用的中间张量组成，这些张量对于生成推理至关重要。在典型的 LM 程序批量执行过程中，存在大量机会在共享公共前缀的多个不同 LLM 调用之间重用 KV 缓存。然而，当前系统缺乏促进这种重用的有效机制，导致不必要的计算和内存浪费。另一个示例是结构化输出（例如 JSON 模式）的受限解码，其中 LLM 的输出被限制为遵循正则表达式定义的特定语法规则（第 4 节）。在这些约束下，多个标记通常可以解码一次。但是，现有系统一次只能解码一个标记，导致解码速度不理想。

 ![image](https://github.com/user-attachments/assets/c4e11b18-a748-4cf5-a81e-fa61a5072e68)


To address these challenges, we present SGLang, a Structured Generation Language for LLMs. The core idea is to systematically exploit the multi-call structure in LM programs for efficient execution. As shown in Fig. 1, it has two parts: a front-end language and a back-end runtime. The front-end simplifies the programming of LM programs, and the runtime accelerates their execution. The two parts can work together for better performance but can also function independently. 

为了应对这些挑战，我们提出了 SGLang，一种用于 LLM 的结构化生成语言。其核心思想是系统地利用 LM 程序中的多调用结构来高效执行。如图 1 所示，它有两个部分：前端语言和后端运行时。前端简化了 LM 程序的编程，而运行时则加速了它们的执行。这两个部分可以协同工作以获得更好的性能，但也可以独立运行。

We introduce SGLang as a domain-specific language embedded in Python. It provides primitives for generation (e.g., extend, gen, select) and parallelism control (e.g., fork, join). SGLang is compatible with Python’s control flow and libraries, so users can develop advanced prompting workflows easily with native Python syntax. We provide an interpreter and a compiler for SGLang. The interpreter manages the prompt state as a stream and submits primitive operations to the stream for asynchronous execution, ensuring proper control over synchronization and intra-program parallelism. Additionally, SGLang program can be traced and compiled for more optimizations.

我们引入了 SGLang 作为嵌入在 Python 中的领域特定语言。它提供用于生成（例如 extend、gen、select）和并行控制（例如 fork、join）的原语。SGLang 与 Python 的控制流和库兼容，因此用户可以使用原生 Python 语法轻松开发高级提示工作流。我们为 SGLang 提供了一个解释器和一个编译器。解释器将提示状态作为流进行管理，并将原语操作提交给流进行异步执行，确保对同步和程序内并行进行适当的控制。此外，可以跟踪和编译 SGLang 程序以进行更多优化。

On the runtime side, we propose several novel optimizations to accelerate the execution of SGLang programs. The first technique, RadixAttention, enables the automatic reuse of the KV cache across multiple generation calls. In existing inference engines, the KV cache of a request is discarded after processing is completed, preventing the KV cache from being reused across multiple calls and significantly slowing down the execution. Instead, our system maintains an LRU cache of the KV cache for all requests within a radix tree. This approach manages the KV cache as a traditional cache and uses a radix tree for efficient matching, insertion, and eviction. It allows the runtime to handle various reuse patterns with a cache-aware scheduling policy efficiently. The second technique is a compressed finite state machine, which enables faster constrained decoding for structured outputs. Existing systems follow the constraints only for the next token by masking probabilities of disallowed tokens, making them able to decode only one token at a time. Instead, our system analyzes the constraints and builds a compressed finite-state machine to represent the constraint. This approach compresses a multi-token path into a single-step path whenever possible, allowing the decoding of multiple tokens at once to achieve faster decoding speed. Lastly, SGLang also supports API-only models like OpenAI’s GPT-4, and we introduce the third technique, API speculative execution, to optimize multi-call programs for API-only models. 

在运行时方面，我们提出了几种新颖的优化来加速 SGLang 程序的执行。第一种技术 RadixAttention 可以在多个生成调用之间自动重用 KV 缓存。在现有的推理引擎中，请求的 KV 缓存在处理完成后被丢弃，从而阻止 KV 缓存在多个调用之间重用，并显著减慢执行速度。相反，我们的系统在基数树中为所有请求维护 KV 缓存的 LRU 缓存。这种方法将 KV 缓存作为传统缓存进行管理，并使用基数树进行有效的匹配、插入和逐出。它允许运行时使用缓存感知调度策略有效地处理各种重用模式。第二种技术是压缩有限状态机，它可以更快地对结构化输出进行约束解码。现有系统通过屏蔽不允许的标记的概率来遵循下一个标记的约束，使它们一次只能解码一个标记。相反，我们的系统分析约束并构建一个压缩有限状态机来表示约束。这种方法尽可能将多 token 路径压缩为单步路径，从而允许一次解码多个 token，以实现更快的解码速度。最后，SGLang 还支持仅 API 模型，如 OpenAI 的 GPT-4，我们引入了第三种技术 API 推测执行，以优化仅 API 模型的多调用程序。

Using SGLang, we implemented various LLM applications, including agent control, logical reasoning, few-shot learning benchmarks, JSON decoding, retrieval-augmented generation pipelines, multiturn chat, and multi-modality processing. We tested the performance on models including Llama7B/70B [49], Mistral-8x7B [17], LLaVA-v1.5-7B (image) [28], and LLaVA-NeXT-34B (video) [62] on NVIDIA A10G and A100 GPUs. Experimental results show that SGLang achieves up to 6.4× higher throughput across a wide range of workloads, models, and hardware setups, compared to existing programming and inference systems, including Guidance [13], vLLM [23], and LMQL [4].

使用 SGLang，我们实现了各种 LLM 应用，包括代理控制、逻辑推理、小样本学习基准、JSON 解码、检索增强生成管道、多轮聊天和多模态处理。我们在 NVIDIA A10G 和 A100 GPU 上测试了包括 Llama7B/70B [49]、Mistral-8x7B [17]、LLaVA-v1.5-7B（图像）[28] 和 LLaVA-NeXT-34B（视频）[62] 在内的模型的性能。实验结果表明，与现有的编程和推理系统（包括 Guidance [13]、vLLM [23] 和 LMQL [4]）相比，SGLang 在各种工作负载、模型和硬件设置中实现了高达 6.4 倍的吞吐量。


# 2.Programming Model

This section introduces the SGLang programming model with a running example, describes its language primitives and execution modes, and outlines runtime optimization opportunities. This programming model can simplify tedious operations in multi-call workflows (e.g., string manipulation, API calling, constraint specification, parallelism) by providing flexible and composable primitives. 

本节通过一个运行示例介绍 SGLang 编程模型，描述其语言原语和执行模式，并概述运行时优化机会。此编程模型可以通过提供灵活且可组合的原语来简化多调用工作流（例如字符串操作、API 调用、约束规范、并行性）中的繁琐操作。

A running example. The language is a domain-specific language embedded in Python. Fig. 2 shows a program that evaluates an essay about an image using the branch-solve-merge prompting method [40]. The function multi_dimensional_judge takes three arguments: s, path, and essay. s manages the prompt state, path is the image file path, and essay is the essay text. New strings and SGLang primitives can be appended to the state s for execution using the += operator. First, the function adds the image and essay to the prompt. It then checks if the essay is related to the image using select, storing the result in s["related"]. If related, the prompt is forked into three copies for parallel evaluation from different dimensions, using gen to store results in f["judgment"]. Next, it merges the judgments, generates a summary, and assigns a letter grade. Finally, it returns the results in JSON format, following a schema defined by a regular expression constraint regex. SGLang greatly simplifies this program, as an equivalent program using an OpenAI API-like interface would take 2.1× as many lines of code due to manual string manipulation and parallelism control. 

正在运行的示例。该语言是嵌入在 Python 中的领域特定语言。图 2 显示了使用分支解决合并提示方法评估关于图像的文章的程序 [40]。函数 multi_Dimension_judge 接受三个参数：s、path 和 essay。s 管理提示状态，path 是图像文件路径，essay 是文章文本。可以使用 += 运算符将新字符串和 SGLang 原语附加到状态 s 以供执行。首先，该函数将图像和文章添加到提示中。然后，它使用 select 检查文章是否与图像相关，并将结果存储在 s["related"] 中。如果相关，则将提示分为三个副本以从不同维度进行并行评估，使用 gen 将结果存储在 f["judgment"] 中。接下来，它合并判断，生成摘要并分配字母等级。最后，它以 JSON 格式返回结果，遵循由正则表达式约束 regex 定义的模式。 SGLang 大大简化了这个程序，因为使用类似 OpenAI API 接口的等效程序由于手动字符串操作和并行控制将需要 2.1 倍的代码行数。

Language primitives. SGLang provides primitives for controlling prompt state, generation, and parallelism. They can be used together with Python syntax and libraries. Here are the primitives: “gen” calls a model to generate and stores the results in a variable with the name specified in its first argument. It supports a “regex” argument to constrain the output to follow a grammar defined by a regular expression (e.g., a JSON schema). “select” calls a model to choose the highest probability option from a list. The operator “+=” or “extend” appends a string to the prompt. The operator “[variable_name]” fetches the results of a generation. “fork” creates parallel forks of the prompt state. “join” rejoins the prompt state. “image” and “video” take in image and video inputs. 

语言原语。SGLang 提供用于控制提示状态、生成和并行性的原语。它们可以与 Python 语法和库一起使用。以下是原语：“gen”调用模型来生成并将结果存储在变量中，变量的名称在其第一个参数中指定。它支持“regex”参数，以约束输出遵循正则表达式定义的语法（例如 JSON 模式）。“select”调用模型从列表中选择最高概率的选项。运算符“+=”或“extend”将字符串附加到提示。运算符“[variable_name]”获取生成的结果。“fork”创建提示状态的并行分支。“join”重新加入提示状态。“image”和“video”接收图像和视频输入。

Execution modes. The simplest way to execute an SGLang program is through an interpreter, where a prompt is treated as an asynchronous stream. Primitives like extend, gen, and select are submitted to the stream for asynchronous execution. These non-blocking calls allow Python code to continue running without waiting for the generation to finish. This is similar to launching CUDA kernels asynchronously. Each prompt is managed by a stream executor in a background thread, enabling intra-program parallelism. Fetching generation results will block until they are ready, ensuring correct synchronization. Alternatively, SGLang programs can be compiled as computational graphs and executed with a graph executor, allowing for more optimizations. This paper uses interpreter mode by default and discusses compiler mode results in Appendix D. SGLang supports open-weight models with its own SGLang Runtime (SRT), as well as API models such as OpenAI and Anthropic models. 

执行模式。执行 SGLang 程序的最简单方法是通过解释器，其中提示被视为异步流。extend、gen 和 select 等原语被提交到流中进行异步执行。这些非阻塞调用允许 Python 代码继续运行，而无需等待生成完成。这类似于异步启动 CUDA 内核。每个提示都由后台线程中的流执行器管理，从而实现程序内并行性。获取生成结果将阻塞，直到它们准备就绪，确保正确同步。或者，SGLang 程序可以编译为计算图并使用图执行器执行，从而实现更多优化。本文默认使用解释器模式，并在附录 D 中讨论编译器模式结果。SGLang 使用自己的 SGLang Runtime (SRT) 支持开放权重模型，以及 OpenAI 和 Anthropic 模型等 API 模型。

![image](https://github.com/user-attachments/assets/13c77525-e59d-49e2-9264-338cf34cb31e)

Comparison. Programming systems for LLMs can be classified as high-level (e.g., LangChain, DSPy) and low-level (e.g., LMQL, Guidance, SGLang). High-level systems provide predefined or auto-generated prompts, such as DSPy’s prompt optimizer. Low-level systems typically do not alter prompts but allow direct manipulation of prompts and primitives. SGLang is a low-level system similar to LMQL and Guidance. Table 1 compares their features. SGLang focuses more on runtime efficiency and comes with its own co-designed runtime, allowing for novel optimizations introduced later. High-level languages (e.g., DSPy) can be compiled to low-level languages (e.g., SGLang). We demonstrate the integration of SGLang as a backend in DSPy for better runtime efficiency in Sec. 6. 

比较。LLM 的编程系统可分为高级（例如 LangChain、DSPy）和低级（例如 LMQL、Guidance、SGLang）。高级系统提供预定义或自动生成的提示，例如 DSPy 的提示优化器。低级系统通常不会更改提示，但允许直接操作提示和原语。SGLang 是一个类似于 LMQL 和 Guidance 的低级系统。表 1 比较了它们的功能。SGLang 更注重运行时效率，并带有自己的共同设计的运行时，允许稍后引入新的优化。高级语言（例如 DSPy）可以编译为低级语言（例如 SGLang）。我们在第 6 节中展示了将 SGLang 作为后端集成到 DSPy 中以提高运行时效率。

![image](https://github.com/user-attachments/assets/41784589-8525-4e1d-b086-cb32ebcf7ec9)

Runtime optimizations. Fig. 2 shows three runtime optimization opportunities: KV cache reuse, fast constrained decoding, API speculative execution. We will discuss them in the following sections.

运行时优化。图 2 显示了三个运行时优化机会：KV 缓存重用、快速约束解码、API 推测执行。我们将在以下部分中讨论它们。

# 3.Efficient KV Cache Reuse with RadixAttention

SGLang programs can chain multiple generation calls and create parallel copies with the "fork" primitive. Additionally, different program instances often share some common parts (e.g., system prompts). These scenarios create many shared prompt prefixes during execution, leading to numerous opportunities for reusing the KV cache. During LLM inference, the KV cache stores intermediate tensors from the forward pass, reused for decoding future tokens. They are named after key-value pairs in the self-attention mechanism [51]. KV cache computation depends only on prefix tokens. Therefore, requests with the same prompt prefix can reuse the KV cache, reducing redundant computation and memory usage. More background and some examples are provided in Appendix A.

SGLang 程序可以链接多个生成调用并使用“fork”原语创建并行副本。此外，不同的程序实例通常共享一些公共部分（例如，系统提示）。这些场景在执行期间创建许多共享的提示前缀，从而为重用 KV 缓存提供了大量机会。在 LLM 推理期间，KV 缓存存储来自前向传递的中间张量，可重用于解码未来的标记。它们以自注意机制中的键值对命名 [51]。KV 缓存计算仅取决于前缀标记。因此，具有相同提示前缀的请求可以重用 KV 缓存，从而减少冗余计算和内存使用。附录 A 中提供了更多背景信息和一些示例。


Given the KV cache reuse opportunity, a key challenge in optimizing SGLang programs is reusing the KV cache across multiple calls and instances. While some systems explore certain KV cache reuse cases [23, 58, 18, 12], they often need manual configurations and cannot handle all reuse patterns (e.g., dynamic tree structures). Consequently, most state-of-the-art inference systems recompute the KV cache for each request. We will discuss their limitations and our differences in Sec. 7.

鉴于 KV 缓存重用机会，优化 SGLang 程序的一个关键挑战是在多个调用和实例中重用 KV 缓存。虽然某些系统探索了某些 KV 缓存重用案例 [23、58、18、12]，但它们通常需要手动配置并且无法处理所有重用模式（例如，动态树结构）。因此，大多数最先进的推理系统都会为每个请求重新计算 KV 缓存。我们将在第 7 节讨论它们的局限性和我们的差异。

This section introduces RadixAttention, a novel technique for automatic and systematic KV cache reuse during runtime. Unlike existing systems that discard the KV cache after a generation request finishes, our system retains the cache for prompts and generation results in a radix tree, enabling efficient prefix search, reuse, insertion, and eviction. We implement an LRU eviction policy and a cache-aware scheduling policy to enhance the cache hit rate. RadixAttention is compatible with techniques like continuous batching [60], paged attention [23], and tensor parallelism [44]. In addition, it introduces only negligible memory and time overhead when there is no cache hit.

本节介绍 RadixAttention，这是一种在运行时自动和系统化 KV 缓存重用的新技术。与现有系统在生成请求完成后丢弃 KV 缓存不同，我们的系统在基数树中保留提示和生成结果的缓存，从而实现高效的前缀搜索、重用、插入和驱逐。我们实施了 LRU 驱逐策略和缓存感知调度策略来提高缓存命中率。RadixAttention 与连续批处理 [60]、分页注意 [23] 和张量并行 [44] 等技术兼容。此外，当没有缓存命中时，它只会引入可忽略不计的内存和时间开销。



RadixAttention. A radix tree is a data structure that serves as a space-efficient alternative to a classical trie (prefix tree). Unlike typical trees, the edges of a radix tree can be labeled not just with single elements but also with sequences of elements of varying lengths, significantly enhancing efficiency. In our system, we utilize a radix tree to manage a mapping between sequences of tokens, and their corresponding KV cache tensors. These KV cache tensors are stored in a non-contiguous, paged layout, where the size of each page is equivalent to one token. Because GPU memory is quickly filled by the KV cahce, we introduce a simple LRU eviction policy that evicts the least recently used leaf first. By evicting leaves first, we enable the re-use of their common ancestors until those ancestors become leaves and are also evicted.

RadixAttention。基数树是一种数据结构，可作为经典 trie（前缀树）的空间高效替代方案。与典型的树不同，基数树的边缘不仅可以用单个元素标记，还可以用不同长度的元素序列标记，从而显着提高效率。在我们的系统中，我们利用基数树来管理标记序列与其对应的 KV 缓存张量之间的映射。这些 KV 缓存张量存储在非连续的分页布局中，其中每页的大小相当于一个标记。由于 GPU 内存很快就会被 KV 缓存填满，因此我们引入了一个简单的 LRU 驱逐策略，该策略首先驱逐最近最少使用的叶子。通过首先驱逐叶子，我们可以重新使用它们的共同祖先，直到这些祖先成为叶子并被驱逐。

 
![image](https://github.com/user-attachments/assets/05ceac1a-37e1-4605-92ee-b15d71fe8c6c)

> 图 3：具有 LRU 驱逐策略的 RadixAttention 操作示例，跨越九个时间点进行说明。该图演示了基数树响应各种请求的动态演变。这些请求包括两个聊天会话、一批少量学习查询和一个自洽采样。每个树边都带有一个标签，表示子字符串或令牌序列。节点采用颜色编码以反映不同的状态：绿色表示新添加的节点，蓝色表示在该时间点访问的缓存节点，红色表示已被驱逐的节点。在步骤 (1) 中，基数树最初为空。在步骤 (2) 中，服务器处理传入的用户消息“Hello”，并以 LLM 输出“Hi”进行响应。系统提示“你是一个乐于助人的助手”、用户消息“Hello！”和 LLM 回复“Hi！”被合并到树中，作为链接到新节点的单个边。在步骤 (3) 中，新提示到达，服务器在基数树中找到提示的前缀（即对话的第一轮），并重用其 KV 缓存。新轮作为新节点附加到树中。在步骤 (4) 中，开始新的聊天会话。来自 (3) 的节点“b”被拆分为两个节点，以允许两个聊天会话共享系统提示。在步骤 (5) 中，第二个聊天会话继续。但是，由于内存限制，必须驱逐来自 (4) 的节点“c”。新轮附加在 (4) 中的节点“d”之后。在步骤 (6) 中，服务器接收少量学习查询，对其进行处理，并将其插入树中。根节点被拆分，因为新查询与现有节点不共享任何前缀。在步骤 (7) 中，服务器接收一批额外的少量学习查询。这些查询共享同一组少量示例，因此我们从 (6) 中拆分节点“e”以实现共享。在步骤 (8) 中，服务器从第一个聊天会话中收到一条新消息。它会逐出第二个聊天会话中的所有节点（节点“g”和“h”），因为它们最近使用最少。在步骤 (9) 中，服务器收到一个请求，要求从 (8) 中为节点“j”中的问题抽取更多答案，这很可能是为了促进自洽。为了给这些请求腾出空间，我们逐出 (8) 中的节点“i”、“k”和“l”。

In the continuous batching setting, we cannot evict nodes used by the currently running batch. Therefore, each node maintains a reference counter indicating how many running requests are using it. A node is evictable if its reference counter is zero. Note that we do not preallocate a fixed-size memory pool as a cache. Instead, we let the cached tokens and the currently running requests share the same memory pool. Therefore, the system dynamically allocates memory for cache and running requests. When enough waiting requests run, the system will evict all cached tokens in favor of a larger batch size. Fig. 3 shows how the radix tree is maintained for several incoming requests. The frontend interpreter sends full prompts to the runtime, and the runtime performs prefix matching and reuse. The tree structure is stored on the CPU with negligible maintenance overhead. During the execution of the fork primitive, the frontend sends the prefix first as a hint, ensuring the prefix is correctly inserted into the tree. It then sends the remaining prompts. This "Frontend Hint" simplifies runtime scheduling and matching, exemplifying the benefits of frontend-runtime co-design.

在连续批处理设置中，我们无法驱逐当前正在运行的批处理所使用的节点。因此，每个节点都维护一个引用计数器，指示有多少正在运行的请求正在使用它。如果节点的引用计数器为零，则该节点是可驱逐的。请注意，我们不会预先分配固定大小的内存池作为缓存。相反，我们让缓存的令牌和当前正在运行的请求共享同一个内存池。因此，系统会动态分配内存用于缓存和正在运行的请求。当有足够多的等待请求运行时，系统将驱逐所有缓存的令牌，以支持更大的批处理大小。图 3 显示了如何为多个传入请求维护基数树。前端解释器将完整提示发送到运行时，运行时执行前缀匹配和重用。树结构存储在 CPU 上，维护开销可忽略不计。在执行 fork 原语期间，前端首先发送前缀作为提示，确保前缀正确插入树中。然后发送剩余的提示。此“前端提示”简化了运行时调度和匹配，体现了前端运行时协同设计的优势。


Cache-aware scheduling. We define the cache hit rate as number of cached prompt tokens number of prompt tokens . When there are many requests in the waiting queue, the order in which they are executed can significantly impact the cache hit rate. For example, if the request scheduler frequently switches between different, unrelated requests, it can lead to cache thrashing and a low hit rate. We design a cache-aware scheduling algorithm to increase the cache hit rate. In the batch-processing setting we sort the requests by matched prefix length and prioritize requests with longer matched prefixes instead of using a first-come, first-served schedule. Alg. 1 (Appendix) shows the pseudo-code for cache-aware scheduling with contiguous batching. The algorithm uses longest-shared-prefix-first order. In more latency-sensitive settings we may still be able to tolerate limited batch re-ordering to improve cache reuse. Additionally, we prove the following theorem for optimal scheduling in the offline case2 

缓存感知调度。我们将缓存命中率定义为缓存提示令牌数 提示令牌数 。当等待队列中有许多请求时，它们的执行顺序会显著影响缓存命中率。例如，如果请求调度程序频繁在不同的、不相关的请求之间切换，则会导致缓存抖动和低命中率。我们设计了一种缓存感知调度算法来提高缓存命中率。在批处理设置中，我们按匹配的前缀长度对请求进行排序，并优先处理具有较长匹配前缀的请求，而不是使用先到先得的调度。算法 1（附录）显示了具有连续批处理的缓存感知调度的伪代码。该算法使用最长共享前缀优先顺序。在对延迟更敏感的设置中，我们可能仍然能够容忍有限的批处理重新排序以提高缓存重用率。此外，我们证明了离线情况 2 中最优调度的以下定理

Theorem 3.1. For a batch of requests, we can achieve an optimal cache hit rate by visiting the radix tree of the requests in the depth-first search order, with a cache size ≥ the maximum request length. The longest-shared-prefix-first order is equivalent to a depth-first search order. The proof is in Sec. A.3 (Appendix). In the online case, the DFS order will be disrupted, but our schedule still approximates the DFS behavior on the augmented part of the full radix tree, as described in Sec. A.3. While greedy cache-aware scheduling can achieve high throughput, it can lead to starvation. We leave its integration with other fair scheduling methods [42] as future work. 

定理 3.1。对于一批请求，我们可以通过按深度优先搜索顺序访问请求的基数树来实现最佳缓存命中率，其中缓存大小≥最大请求长度。最长共享前缀优先顺序等同于深度优先搜索顺序。证明在 A.3 节（附录）中。在线情况下，DFS 顺序将被打乱，但我们的调度仍然近似于完整基数树的增强部分上的 DFS 行为，如 A.3 节所述。虽然贪婪的缓存感知调度可以实现高吞吐量，但它可能导致饥饿。我们将其与其他公平调度方法 [42] 的集成留待未来研究。

Distributed Cases. RadixAttention can be extended to multiple GPUs. For tensor parallelism, each GPU maintains a sharded KV cache. There is no need for additional synchronization because the tree operations are the same. Data parallelism with multiple workers is discussed in Sec. A.4 (Appendix).

分布式案例。RadixAttention 可以扩展到多个 GPU。对于张量并行，每个 GPU 维护一个分片的 KV 缓存。由于树操作相同，因此不需要额外的同步。第 3.3 节讨论了具有多个工作器的数据并行性。 A.4（附录）。

# 4.Efficient Constrained Decoding with Compressed Finite State Machine

 ![image](https://github.com/user-attachments/assets/09fe6178-e54a-41dd-84e5-d946f1a0fcf7)


In LM programs, users often want to constrain the model’s output to follow specific formats, such as JSON schemas. This can improve controllability and robustness, and make the output easier to parse. SGLang offers a regex argument to enforce such constraints using regular expressions, which are expressive enough for many practical scenarios. Existing systems support this by converting a regular expression into a finite state machine (FSM) [54]. During decoding, they maintain the current FSM state, retrieve allowed tokens from the next states, and set the probability of invalid tokens to zero, decoding token by token. This token-by-token approach, however, is inefficient when there are opportunities to decode multiple tokens at once. For example, the constant sequence {"summary": " in Fig. 2 spans multiple tokens in the normal decoding process as shown in Fig. 4 (c), requiring multiple decoding stages, even though there is only one valid next token when decoding it. Therefore, the whole sequence can be decoded in a single step (i.e., forward pass). However, existing systems can only decode one token at a time because the lack of integration between the FSM and the model runner in existing systems prevents multi-token processing, resulting in slow decoding. 

在 LM 程序中，用户通常希望将模型的输出限制为遵循特定格式，例如 JSON 模式。这可以提高可控性和鲁棒性，并使输出更易于解析。SGLang 提供了一个 regex 参数，使用正则表达式来强制执行此类约束，这些正则表达式对于许多实际场景来说已经足够具有表现力。现有系统通过将正则表达式转换为有限状态机 (FSM) [54] 来支持这一点。在解码过程中，它们保持当前 FSM 状态，从下一个状态检索允许的标记，并将无效标记的概率设置为零，逐个标记地解码。但是，当有机会同时解码多个标记时，这种逐个标记的方法效率低下。例如，图 2 中的常量序列 {"summary": " 在正常解码过程中跨越多个 token（如图 4 (c) 所示），需要多个解码阶段，即使在解码时只有一个有效的下一个 token。因此，整个序列可以在一个步骤（即正向传递）中解码。但是，现有系统一次只能解码一个 token，因为现有系统中 FSM 和模型运行器之间缺乏集成，阻止了多 token 处理，导致解码速度缓慢。

SGLang overcomes this limitation by creating a fast constrained decoding runtime with a compressed FSM. This runtime analyzes the FSM and compresses adjacent singular-transition edges in the FSM into single edges as demonstrated in Fig. 4 (b), allowing it to recognize when multiple tokens can be decoded together. In Fig. 4 (d), multiple tokens on the compressed transition edge can be decoded in one forward pass, which greatly accelerates the decoding process. It is also general and applicable to all regular expressions. More details on the background and implementation are in Appendix B.

SGLang 通过创建具有压缩 FSM 的快速约束解码运行时来克服这一限制。此运行时分析 FSM 并将 FSM 中相邻的奇异转换边压缩为单个边，如图 4 (b) 所示，使其能够识别何时可以一起解码多个 token。在图 4 (d) 中，压缩转换边上的多个 token 可以在一次正向传递中解码，这大大加快了解码过程。它也是通用的，适用于所有正则表达式。有关背景和实施的更多详细信息请参阅附录 B。

# 5.Efficient Endpoint Calling with API Speculative Execution

The previous sections introduced optimizations for open-weight models, which require modifications to the model inference process. Additionally, SGLang works with API-access-only models, such as OpenAI’s GPT-4. However, for these models, we can only call a black-box API endpoint. 

This section introduces a new optimization for black-box API models that accelerates execution and reduces the API cost of multi-call SGLang programs using speculative execution. For example, a program may ask the model to generate a description of a character with a multi-call pattern: s += context + "name:" + gen("name", stop="\n") + "job:" + gen("job", stop="\n"). Naively, the two gen primitives correspond to two API calls, meaning that the user needs to pay for the input token fee on the context twice. In SGLang, we can enable speculative execution on the first call and let it continue the generation of a few more tokens by ignoring the stop condition. The interpreter keeps the additional generation outputs and matches and reuses them with later primitives. In certain cases, with careful prompt engineering, the model can correctly match the template with high accuracy, saving us the latency and input costs of one API call.

前面几节介绍了针对开放权重模型的优化，这需要修改模型推理过程。此外，SGLang 还适用于仅 API 访问的模型，例如 OpenAI 的 GPT-4。但是，对于这些模型，我们只能调用黑盒 API 端点。

本节介绍了一种针对黑盒 API 模型的新优化，该优化使用推测执行来加速执行并降低多调用 SGLang 程序的 API 成本。例如，程序可能会要求模型使用多调用模式生成字符描述：s += context + "name:" + gen("name", stop="\n") + "job:" + gen("job", stop="\n")。简单来说，两个 gen 原语对应两个 API 调用，这意味着用户需要在 context 上支付两次输入 token 费用。在 SGLang 中，我们可以在第一次调用时启用推测执行，并让它通过忽略停止条件继续生成更多 token。解释器会保留额外的生成输出，并与后续的基元进行匹配和重用。在某些情况下，通过仔细的快速工程，模型可以以高精度正确匹配模板，从而为我们节省一次 API 调用的延迟和输入成本。

# 6. Evaluation

# 7. Related Work

Various works have explored the reuse of the KV cache, and many of them are concurrent with our work. Uniquely, our RadixAttention first proposes treating the KV cache as a tree-based LRU cache. It is the first solution that supports multi-level sharing, cache-aware scheduling, frontend-runtime co-scheduling, and distributed cases. vLLM [23] and ChunkedAttention [58] explore some simple reuse cases (e.g., system prompt sharing) but do not cover multi-level tree-structured sharing or LRU caching. PromptCache [12] proposes the modular reuse of the KV cache beyond the prefix but can impact accuracy by up to a 43% drop. HydraGen [18], FlashInfer [59], and ChunkedAttention [58] focus on CUDA kernel optimizations and do not include the concept of an LRU cache. API Serve [1] and LLM-SQL [29] study KV cache reuse for specific applications such as interleaving with external API calls and relational databases, but they do not have our radix tree or cache-aware scheduling. 9 Several LLM programming and agent frameworks exist, such as Guidance [13], LMQL [4], DSPy [20], LangChain [24], AutoGen [55], and LLM Compiler [21]. Guidance and LMQL are most similar to SGLang, and we compare them in Sec. 2. Our innovation lies in novel runtime optimizations for accelerating the proposed programming model. SGLang is compatible with other frameworks and can accelerate them (e.g., the DSPy example in our evaluation). Additionally, SGLang is compatible with many other common inference optimizations [60, 39, 3, 23, 59, 10, 26, 15, 19, 32, 31, 11].

各种研究都探索了 KV 缓存的重用，其中许多与我们的工作同时进行。独特的是，我们的 RadixAttention 首先提出将 KV 缓存视为基于树的 LRU 缓存。它是第一个支持多级共享、缓存感知调度、前端运行时协同调度和分布式情况的解决方案。vLLM [23] 和 ChunkedAttention [58] 探索了一些简单的重用案例（例如，系统提示共享），但不涵盖多级树结构共享或 LRU 缓存。PromptCache [12] 提出了前缀之外的 KV 缓存的模块化重用，但可能会使准确率下降高达 43%。HydraGen [18]、FlashInfer [59] 和 ChunkedAttention [58] 专注于 CUDA 内核优化，不包括 LRU 缓存的概念。 API Serve [1] 和 LLM-SQL [29] 研究了特定应用（例如与外部 API 调用和关系数据库交错）的 KV 缓存重用，但它们没有我们的基数树或缓存感知调度。9 存在几种 LLM 编程和代理框架，例如 Guidance [13]、LMQL [4]、DSPy [20]、LangChain [24]、AutoGen [55] 和 LLM Compiler [21]。Guidance 和 LMQL 与 SGLang 最为相似，我们在第 2 节中对它们进行了比较。我们的创新在于新颖的运行时优化，以加速所提出的编程模型。SGLang 与其他框架兼容并可以加速它们（例如，我们评估中的 DSPy 示例）。此外，SGLang 与许多其他常见的推理优化兼容[60、39、3、23、59、10、26、15、19、32、31、11]。

# Additional Details on RadixAttention

A.1 Background on the KV Cache
Most LLMs in use today, such as GPT-3 [5], PaLM [9], and LLaMA [49], are based on the autoregressive Transformer architecture [51]. These models predict the probability of the next token in a sequence based on the preceding tokens. During inference, the model first processes a sequence of input tokens through a forward pass (this process is called "prefill"). It then sequentially decodes output tokens, with each token depending on prior tokens (this process is called "decoding"). We refer to the process of taking a sequence of input tokens and generating a sequence of output tokens as a single-generation call. Throughout this process, each token generates some intermediate tensors, which are used for decoding further tokens. These intermediate tensors, known as the "KV Cache," are named for the key-value pairs in the self-attention mechanism. An important observation when discussing optimizations in this paper is that the computation of the KV cache only depends on all previous tokens, so different sequences with the same prefix can reuse the KV cache of the prefix tokens and avoid redundant computation. Often in LLM programs, multiple text segments and generation calls are appended to a single prompt. Caching the computed KV cache for previous tokens across multiple chained calls can reduce redundant computation. This optimization, however, is neither free nor trivial, as it requires additional storage and more complex memory management. In addition, it is common in LLM programs to generate multiple outputs from a single prompt or to fork a new prompt from the current state [25]. Basic prefix sharing has been investigated in vLLM [23]. More advanced sharing patterns like irregular tree-structured sharing can also be employed. Fig. 9 shows four typical patterns of KV cache sharing across multiple calls; none of the existing systems can automatically handle all of them. On the contrary, our RadixAttention in Sec. 3 can handle all of them automatically at runtime.

目前使用的大多数 LLM，例如 GPT-3 [5]、PaLM [9] 和 LLaMA [49]，都是基于自回归 Transformer 架构 [51]。这些模型根据前面的 token 预测序列中下一个 token 的概率。在推理过程中，模型首先通过前向传递处理输入 token 序列（此过程称为“预填充”）。然后，它按顺序解码输出 token，每个 token 都依赖于先前的 token（此过程称为“解码”）。我们将获取输入 token 序列并生成输出 token 序列的过程称为单代调用。在此过程中，每个 token 都会生成一些中间张量，用于解码进一步的 token。这些中间张量称为“KV 缓存”，以自注意力机制中的键值对命名。本文讨论优化时的一个重要观察是，KV 缓存的计算仅依赖于所有先前的标记，因此具有相同前缀的不同序列可以重用前缀标记的 KV 缓存并避免冗余计算。在 LLM 程序中，多个文本段和生成调用通常会附加到单个提示中。在多个链式调用中缓存先前标记的计算 KV 缓存可以减少冗余计算。然而，这种优化既不免费也不简单，因为它需要额外的存储和更复杂的内存管理。此外，在 LLM 程序中，从单个提示生成多个输出或从当前状态分叉新提示是很常见的 [25]。vLLM [23] 中研究了基本前缀共享。还可以采用更高级的共享模式，如不规则树结构共享。图 9 展示了跨多个调用共享 KV 缓存的四种典型模式；现有的系统都无法自动处理所有这些模式。相反，我们在第 3 节中的 RadixAttention 可以在运行时自动处理所有这些模式。



