# DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model

https://arxiv.org/pdf/2405.04434

# Abstract
  
We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V2.

æˆ‘ä»¬æå‡ºäº† DeepSeek-V2ï¼Œä¸€ç§å¼ºå¤§çš„æ··åˆä¸“å®¶ (MoE) è¯­è¨€æ¨¡å‹ï¼Œå…·æœ‰è®­ç»ƒç»æµã€æ¨ç†é«˜æ•ˆçš„ç‰¹ç‚¹ã€‚å®ƒåŒ…å« 236B æ€»å‚æ•°ï¼Œå…¶ä¸­æ¯ä¸ª token æ¿€æ´» 21Bï¼Œæ”¯æŒ 128K token çš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚DeepSeek-V2 é‡‡ç”¨äº†åŒ…æ‹¬å¤šå¤´æ½œåœ¨æ³¨æ„åŠ› (MLA) å’Œ DeepSeekMoE åœ¨å†…çš„åˆ›æ–°æ¶æ„ã€‚MLA é€šè¿‡å°†é”®å€¼ (KV) ç¼“å­˜æ˜¾è‘—å‹ç¼©ä¸ºæ½œåœ¨å‘é‡æ¥ä¿è¯é«˜æ•ˆæ¨ç†ï¼Œè€Œ DeepSeekMoE é€šè¿‡ç¨€ç–è®¡ç®—ä»¥ç»æµçš„æˆæœ¬è®­ç»ƒå¼ºå¤§çš„æ¨¡å‹ã€‚ä¸ DeepSeek 67B ç›¸æ¯”ï¼ŒDeepSeek-V2 å®ç°äº†æ˜¾è‘—å¢å¼ºçš„æ€§èƒ½ï¼ŒåŒæ—¶èŠ‚çœäº† 42.5% çš„è®­ç»ƒæˆæœ¬ã€å‡å°‘äº† 93.3% çš„ KV ç¼“å­˜ã€å¹¶å°†æœ€å¤§ç”Ÿæˆååé‡æå‡è‡³ 5.76 å€ã€‚æˆ‘ä»¬åœ¨ç”± 8.1T æ ‡è®°ç»„æˆçš„é«˜è´¨é‡å¤šæºè¯­æ–™åº“ä¸Šå¯¹ DeepSeek-V2 è¿›è¡Œäº†é¢„è®­ç»ƒï¼Œå¹¶è¿›ä¸€æ­¥æ‰§è¡Œç›‘ç£å¾®è°ƒ (SFT) å’Œå¼ºåŒ–å­¦ä¹  (RL) ä»¥å……åˆ†é‡Šæ”¾å…¶æ½œåŠ›ã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå³ä½¿åªæœ‰ 21B æ¿€æ´»å‚æ•°ï¼ŒDeepSeek-V2 åŠå…¶èŠå¤©ç‰ˆæœ¬ä»ç„¶åœ¨å¼€æºæ¨¡å‹ä¸­å®ç°äº†é¡¶çº§æ€§èƒ½ã€‚æ¨¡å‹æ£€æŸ¥ç‚¹å¯åœ¨ https://github.com/deepseek-ai/DeepSeek-V2 ä¸Šæ‰¾åˆ°ã€‚

# 1 Introduction
  
In the past few years, Large Language Models (LLMs) (Anthropic, 2023; Google, 2023; OpenAI, 2022, 2023) have undergone rapid development, offering a glimpse into the dawn of Artificial General Intelligence (AGI). In general, the intelligence of an LLM tends to improve as the number of parameters increases, allowing it to exhibit emergent capabilities across various tasks (Wei et al., 2022). However, the improvement comes at the cost of larger computing resources for training and a potential decrease in inference throughput. These constraints present significant challenges that impede the widespread adoption and utilization of LLMs. In order to tackle this problem, we introduce DeepSeek-V2, a strong open-source Mixture-of-Experts (MoE) language model, characterized by economical training and efficient inference through an innovative Transformer architecture. It is equipped with a total of 236B parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. 

åœ¨è¿‡å»çš„å‡ å¹´ä¸­ï¼Œå¤§å‹è¯­è¨€æ¨¡å‹ (LLM) (Anthropicï¼Œ2023 å¹´ï¼›Googleï¼Œ2023 å¹´ï¼›OpenAIï¼Œ2022 å¹´ï¼Œ2023 å¹´) ç»å†äº†å¿«é€Ÿå‘å±•ï¼Œè®©æˆ‘ä»¬å¾—ä»¥ä¸€çª¥é€šç”¨äººå·¥æ™ºèƒ½ (AGI) çš„æ›™å…‰ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒLLM çš„æ™ºèƒ½å¾€å¾€ä¼šéšç€å‚æ•°æ•°é‡çš„å¢åŠ è€Œæé«˜ï¼Œä»è€Œä½¿å…¶èƒ½å¤Ÿåœ¨å„ç§ä»»åŠ¡ä¸­å±•ç°å‡ºæ–°å…´çš„èƒ½åŠ› (Wei et al.ï¼Œ2022 å¹´)ã€‚ç„¶è€Œï¼Œè¿™ç§æ”¹è¿›æ˜¯ä»¥æ›´å¤§çš„è®­ç»ƒè®¡ç®—èµ„æºå’Œæ¨ç†ååé‡çš„æ½œåœ¨ä¸‹é™ä¸ºä»£ä»·çš„ã€‚è¿™äº›é™åˆ¶å¸¦æ¥äº†é‡å¤§æŒ‘æˆ˜ï¼Œé˜»ç¢äº† LLM çš„å¹¿æ³›é‡‡ç”¨å’Œåˆ©ç”¨ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæˆ‘ä»¬æ¨å‡ºäº† DeepSeek-V2ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„å¼€æºæ··åˆä¸“å®¶ (MoE) è¯­è¨€æ¨¡å‹ï¼Œå…¶ç‰¹ç‚¹æ˜¯é€šè¿‡åˆ›æ–°çš„ Transformer æ¶æ„è¿›è¡Œç»æµçš„è®­ç»ƒå’Œé«˜æ•ˆçš„æ¨ç†ã€‚å®ƒæ€»å…±é…å¤‡äº†236Bä¸ªå‚æ•°ï¼Œå…¶ä¸­æ¯ä¸ªtokenæ¿€æ´»21Bï¼Œæ”¯æŒ128Kä¸ªtokençš„ä¸Šä¸‹æ–‡é•¿åº¦ã€‚

We optimize the attention modules and Feed-Forward Networks (FFNs) within the Transformer framework (Vaswani et al., 2017) with our proposed Multi-head Latent Attention (MLA) and DeepSeekMoE. (1) In the context of attention mechanisms, the Key-Value (KV) cache of the Multi-Head Attention (MHA) (Vaswani et al., 2017) poses a significant obstacle to the inference efficiency of LLMs. Various approaches have been explored to address this issue, including Grouped-Query Attention (GQA) (Ainslie et al., 2023) and Multi-Query Attention (MQA) (Shazeer, 2019). However, these methods often compromise performance in their attempt to reduce the KV cache. In order to achieve the best of both worlds, we introduce MLA, an attention mechanism equipped with low-rank key-value joint compression. Empirically, MLA achieves superior performance compared with MHA, and meanwhile significantly reduces the KV cache during inference, thus boosting the inference efficiency. (2) For Feed-Forward Networks (FFNs), we follow the DeepSeekMoE architecture (Dai et al., 2024), which adopts fine-grained expert segmentation and shared expert isolation for higher potential in expert specialization. The DeepSeekMoE architecture demonstrates great advantages compared with conventional MoE architectures like GShard (Lepikhin et al., 2021), enabling us to train strong models at an economical cost. As we employ expert parallelism during training, we also devise supplementary mechanisms to control communication overheads and ensure load balance. By combining these two techniques, DeepSeek-V2 features strong performance (Figure 1(a)), economical training costs, and efficient inference throughput (Figure 1(b)), simultaneously

æˆ‘ä»¬åˆ©ç”¨æˆ‘ä»¬æå‡ºçš„å¤šå¤´æ½œåœ¨æ³¨æ„åŠ› (MLA) å’Œ DeepSeekMoE ä¼˜åŒ–äº† Transformer æ¡†æ¶ (Vaswani et al., 2017) ä¸­çš„æ³¨æ„åŠ›æ¨¡å—å’Œå‰é¦ˆç½‘ç»œ (FFN)ã€‚ï¼ˆ1ï¼‰åœ¨æ³¨æ„åŠ›æœºåˆ¶çš„èƒŒæ™¯ä¸‹ï¼Œå¤šå¤´æ³¨æ„åŠ› (MHA) (Vaswani et al., 2017) çš„é”®å€¼ (KV) ç¼“å­˜å¯¹ LLM çš„æ¨ç†æ•ˆç‡æ„æˆäº†é‡å¤§éšœç¢ã€‚å·²ç»æ¢ç´¢äº†å„ç§æ–¹æ³•æ¥è§£å†³è¿™ä¸ªé—®é¢˜ï¼ŒåŒ…æ‹¬åˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ› (GQA) (Ainslie et al., 2023) å’Œå¤šæŸ¥è¯¢æ³¨æ„åŠ› (MQA) (Shazeer, 2019)ã€‚ç„¶è€Œï¼Œè¿™äº›æ–¹æ³•åœ¨å°è¯•å‡å°‘ KV ç¼“å­˜æ—¶å¾€å¾€ä¼šæŸå®³æ€§èƒ½ã€‚ä¸ºäº†å®ç°ä¸¤å…¨å…¶ç¾ï¼Œæˆ‘ä»¬å¼•å…¥äº† MLAï¼Œä¸€ç§é…å¤‡ä½ç§©é”®å€¼è”åˆå‹ç¼©çš„æ³¨æ„åŠ›æœºåˆ¶ã€‚ç»éªŒä¸Šï¼ŒMLA ç›¸æ¯” MHA å–å¾—äº†æ›´ä¼˜å¼‚çš„æ€§èƒ½ï¼ŒåŒæ—¶æ˜¾è‘—å‡å°‘äº†æ¨ç†è¿‡ç¨‹ä¸­çš„ KV ç¼“å­˜ï¼Œä»è€Œæé«˜äº†æ¨ç†æ•ˆç‡ã€‚ï¼ˆ2ï¼‰å¯¹äºå‰é¦ˆç½‘ç»œï¼ˆFFNï¼‰ï¼Œæˆ‘ä»¬éµå¾ª DeepSeekMoE æ¶æ„ï¼ˆDai et al., 2024ï¼‰ï¼Œè¯¥æ¶æ„é‡‡ç”¨ç»†ç²’åº¦ä¸“å®¶ç»†åˆ†å’Œå…±äº«ä¸“å®¶éš”ç¦»ï¼Œä»¥å®ç°æ›´é«˜çš„ä¸“å®¶ä¸“ä¸šåŒ–æ½œåŠ›ã€‚ä¸ GShardï¼ˆLepikhin et al., 2021ï¼‰ç­‰ä¼ ç»Ÿ MoE æ¶æ„ç›¸æ¯”ï¼ŒDeepSeekMoE æ¶æ„è¡¨ç°å‡ºå·¨å¤§ä¼˜åŠ¿ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿä»¥ç»æµçš„æˆæœ¬è®­ç»ƒå‡ºå¼ºå¤§çš„æ¨¡å‹ã€‚ç”±äºæˆ‘ä»¬åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­é‡‡ç”¨äº†ä¸“å®¶å¹¶è¡Œï¼Œæˆ‘ä»¬è¿˜è®¾è®¡äº†è¡¥å……æœºåˆ¶æ¥æ§åˆ¶é€šä¿¡å¼€é”€å¹¶ç¡®ä¿è´Ÿè½½å¹³è¡¡ã€‚é€šè¿‡ç»“åˆè¿™ä¸¤ç§æŠ€æœ¯ï¼ŒDeepSeek-V2 å…¼å…·å¼ºå¤§çš„æ€§èƒ½ï¼ˆå›¾ 1(a)ï¼‰ã€ç»æµçš„è®­ç»ƒæˆæœ¬å’Œé«˜æ•ˆçš„æ¨ç†ååé‡ï¼ˆå›¾ 1(b)ï¼‰ï¼Œ

We construct a high-quality and multi-source pre-training corpus consisting of 8.1T tokens. Compared with the corpus used in DeepSeek 67B (our previous release) (DeepSeek-AI, 2024), this corpus features an extended amount of data, especially Chinese data, and higher data quality. We first pretrain DeepSeek-V2 on the full pre-training corpus. Then, we collect 1.5M conversational sessions, which encompass various domains such as math, code, writing, reasoning, safety, and more, to perform Supervised Fine-Tuning (SFT) for DeepSeek-V2 Chat (SFT). Finally, we follow DeepSeekMath (Shao et al., 2024) to employ Group Relative Policy Optimization (GRPO) to further align the model with human preference and produce DeepSeek-V2 Chat (RL). 

æˆ‘ä»¬æ„å»ºäº†ä¸€ä¸ªç”± 8.1T æ ‡è®°ç»„æˆçš„é«˜è´¨é‡ã€å¤šæºé¢„è®­ç»ƒè¯­æ–™åº“ã€‚ä¸ DeepSeek 67Bï¼ˆæˆ‘ä»¬ä¹‹å‰çš„ç‰ˆæœ¬ï¼‰ä¸­ä½¿ç”¨çš„è¯­æ–™åº“ï¼ˆDeepSeek-AIï¼Œ2024ï¼‰ç›¸æ¯”ï¼Œè¯¥è¯­æ–™åº“çš„æ•°æ®é‡æ›´å¤§ï¼Œå°¤å…¶æ˜¯ä¸­æ–‡æ•°æ®ï¼Œæ•°æ®è´¨é‡æ›´é«˜ã€‚æˆ‘ä»¬é¦–å…ˆåœ¨å®Œæ•´çš„é¢„è®­ç»ƒè¯­æ–™åº“ä¸Šå¯¹ DeepSeek-V2 è¿›è¡Œé¢„è®­ç»ƒã€‚ç„¶åï¼Œæˆ‘ä»¬æ”¶é›†äº† 1.5M ä¸ªå¯¹è¯ä¼šè¯ï¼Œæ¶µç›–æ•°å­¦ã€ä»£ç ã€å†™ä½œã€æ¨ç†ã€å®‰å…¨ç­‰å„ä¸ªé¢†åŸŸï¼Œä»¥å¯¹ DeepSeek-V2 Chatï¼ˆSFTï¼‰è¿›è¡Œç›‘ç£å¾®è°ƒï¼ˆSFTï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬éµå¾ª DeepSeekMathï¼ˆShao ç­‰äººï¼Œ2024ï¼‰ï¼Œé‡‡ç”¨ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ˆGRPOï¼‰è¿›ä¸€æ­¥ä½¿æ¨¡å‹ä¸äººç±»åå¥½ä¿æŒä¸€è‡´ï¼Œå¹¶ç”Ÿæˆ DeepSeek-V2 Chatï¼ˆRLï¼‰ã€‚


We evaluate DeepSeek-V2 on a wide range of benchmarks in English and Chinese, and compare it with representative open-source models. Evaluation results show that even with only 21B activated parameters, DeepSeek-V2 still achieves top-tier performance among open-source models and becomes the strongest open-source MoE language model. Figure 1(a) highlights that, on MMLU, DeepSeek-V2 achieves top-ranking performance with only a small number of activated parameters. In addition, as shown in Figure 1(b), compared with DeepSeek 67B, DeepSeek-V2 saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We also evaluate DeepSeek-V2 Chat (SFT) and DeepSeek-V2 Chat (RL) on open-ended benchmarks. Notably, DeepSeek-V2 Chat (RL) achieves 38.9 length-controlled win rate on AlpacaEval 2.0 (Dubois et al., 2024), 8.97 overall score on MT-Bench (Zheng et al., 2023), and 7.91 overall score on AlignBench (Liu et al., 2023). The English open-ended conversation evaluations demonstrate that DeepSeek-V2 Chat (RL) has toptier performance among open-source chat models. In addition, the evaluation on AlignBench indicates that in Chinese, DeepSeek-V2 Chat (RL) outperforms all of open-source models, and even beats most of closed-source models.

æˆ‘ä»¬å¯¹è‹±æ–‡å’Œä¸­æ–‡çš„å¤§é‡ Benchmark æµ‹è¯•é›†è¿›è¡Œäº† DeepSeek-V2 è¯„ä¼°ï¼Œå¹¶ä¸å…·æœ‰ä»£è¡¨æ€§çš„å¼€æºæ¨¡å‹è¿›è¡Œäº†æ¯”è¾ƒã€‚è¯„ä¼°ç»“æœè¡¨æ˜ï¼Œå³ä¾¿åœ¨ä»…æœ‰ 21B æ¿€æ´»å‚æ•°çš„æƒ…å†µä¸‹ï¼ŒDeepSeek-V2 ä¾ç„¶å–å¾—äº†å¼€æºæ¨¡å‹ä¸­çš„é¡¶çº§æ€§èƒ½ï¼Œæˆä¸ºæœ€å¼ºçš„å¼€æº MoE è¯­è¨€æ¨¡å‹ã€‚å›¾ 1(a) è¡¨æ˜ï¼Œåœ¨ MMLU ä¸Šï¼ŒDeepSeek-V2 ä»…ä½¿ç”¨å°‘é‡æ¿€æ´»å‚æ•°ä¾¿å–å¾—äº†é¡¶çº§æ€§èƒ½ã€‚æ­¤å¤–ï¼Œå¦‚å›¾ 1(b) æ‰€ç¤ºï¼Œä¸ DeepSeek çš„ 67B ç›¸æ¯”ï¼ŒDeepSeek-V2 èŠ‚çœäº† 42.5% çš„è®­ç»ƒæˆæœ¬ï¼Œå‡å°‘äº† 93.3% çš„ KV ç¼“å­˜ï¼Œå¹¶å°†æœ€å¤§ç”Ÿæˆååé‡æå‡è‡³ 5.76 å€ã€‚æˆ‘ä»¬è¿˜åœ¨å¼€æ”¾å¼ Benchmark æµ‹è¯•é›†ä¸­è¯„ä¼°äº† DeepSeek-V2 Chat (SFT) å’Œ DeepSeek-V2 Chat (RL)ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒDeepSeek-V2 Chat (RL) åœ¨ AlpacaEval 2.0ï¼ˆDubois ç­‰ï¼Œ2024ï¼‰ä¸Šå®ç°äº† 38.9 çš„é•¿åº¦æ§åˆ¶èƒœç‡ï¼Œåœ¨ MT-Benchï¼ˆZheng ç­‰ï¼Œ2023ï¼‰ä¸Šå®ç°äº† 8.97 çš„æ€»åˆ†ï¼Œåœ¨ AlignBenchï¼ˆLiu ç­‰ï¼Œ2023ï¼‰ä¸Šå®ç°äº† 7.91 çš„æ€»åˆ†ã€‚è‹±æ–‡å¼€æ”¾å¼å¯¹è¯è¯„æµ‹è¡¨æ˜ï¼ŒDeepSeek-V2 Chat (RL) åœ¨å¼€æºèŠå¤©æ¨¡å‹ä¸­æ‹¥æœ‰é¡¶çº§è¡¨ç°ã€‚æ­¤å¤–ï¼ŒAlignBench çš„è¯„æµ‹è¡¨æ˜ï¼Œåœ¨ä¸­æ–‡ä¸­ï¼ŒDeepSeek-V2 Chat (RL) çš„è¡¨ç°ä¼˜äºæ‰€æœ‰å¼€æºæ¨¡å‹ï¼Œç”šè‡³è¶…è¶Šäº†å¤§å¤šæ•°é—­æºæ¨¡å‹ã€‚

In order to facilitate further research and development on MLA and DeepSeekMoE, we also release DeepSeek-V2-Lite, a smaller model equipped with MLA and DeepSeekMoE, for the open-source community. It has a total of 15.7B parameters, where 2.4B are activated for each token. Detailed descriptions about DeepSeek-V2-Lite can be found in Appendix B.

ä¸ºäº†ä¿ƒè¿›å¯¹ MLA å’Œ DeepSeekMoE çš„è¿›ä¸€æ­¥ç ”ç©¶å’Œå¼€å‘ï¼Œæˆ‘ä»¬è¿˜å‘å¼€æºç¤¾åŒºå‘å¸ƒäº† DeepSeek-V2-Liteï¼Œè¿™æ˜¯ä¸€ä¸ªé…å¤‡ MLA å’Œ DeepSeekMoE çš„è¾ƒå°æ¨¡å‹ã€‚å®ƒæ€»å…±æœ‰ 15.7B ä¸ªå‚æ•°ï¼Œå…¶ä¸­æ¯ä¸ª token æ¿€æ´» 2.4Bã€‚æœ‰å…³ DeepSeek-V2-Lite çš„è¯¦ç»†æè¿°å¯åœ¨é™„å½• B ä¸­æ‰¾åˆ°ã€‚

In the rest of this paper, we first provide a detailed description of the model architecture of DeepSeek-V2 (Section 2). Subsequently, we introduce our pre-training endeavors, including the training data construction, hyper-parameter settings, infrastructures, long context extension, and the evaluation of model performance and efficiency (Section 3). Following this, we demonstrate our efforts in alignment, encompassing Supervised Fine-Tuning (SFT), Reinforcement Learning (RL), the evaluation results, and other discussion (Section 4). Finally, we summarize the conclusion, deliberate on the current limitations of DeepSeek-V2, and outline our future work (Section 5).

åœ¨æœ¬æ–‡çš„å…¶ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬é¦–å…ˆè¯¦ç»†æè¿°äº† DeepSeek-V2 çš„æ¨¡å‹æ¶æ„ï¼ˆç¬¬ 2 èŠ‚ï¼‰ã€‚éšåï¼Œæˆ‘ä»¬ä»‹ç»äº†æˆ‘ä»¬çš„é¢„è®­ç»ƒå·¥ä½œï¼ŒåŒ…æ‹¬è®­ç»ƒæ•°æ®æ„å»ºã€è¶…å‚æ•°è®¾ç½®ã€åŸºç¡€è®¾æ–½ã€é•¿ä¸Šä¸‹æ–‡æ‰©å±•ä»¥åŠæ¨¡å‹æ€§èƒ½å’Œæ•ˆç‡çš„è¯„ä¼°ï¼ˆç¬¬ 3 èŠ‚ï¼‰ã€‚éšåï¼Œæˆ‘ä»¬å±•ç¤ºäº†æˆ‘ä»¬åœ¨åè°ƒæ–¹é¢çš„åŠªåŠ›ï¼ŒåŒ…æ‹¬ç›‘ç£å¾®è°ƒ (SFT)ã€å¼ºåŒ–å­¦ä¹  (RL)ã€è¯„ä¼°ç»“æœå’Œå…¶ä»–è®¨è®ºï¼ˆç¬¬ 4 èŠ‚ï¼‰ã€‚æœ€åï¼Œæˆ‘ä»¬æ€»ç»“ç»“è®ºï¼Œè®¨è®º DeepSeek-V2 å½“å‰çš„å±€é™æ€§ï¼Œå¹¶æ¦‚è¿°æˆ‘ä»¬æœªæ¥çš„å·¥ä½œï¼ˆç¬¬ 5 èŠ‚ï¼‰ã€‚

# 2. Architecture

By and large, DeepSeek-V2 is still in the Transformer architecture (Vaswani et al., 2017), where each Transformer block consists of an attention module and a Feed-Forward Network (FFN). However, for both the attention module and the FFN, we design and employ innovative architectures. For attention, we design MLA, which utilizes low-rank key-value joint compression to eliminate the bottleneck of inference-time key-value cache, thus supporting efficient inference. For FFNs, we adopt the DeepSeekMoE architecture (Dai et al., 2024), a high-performance MoE architecture that enables training strong models at an economical cost. An illustration of the architecture of DeepSeek-V2 is presented in Figure 2, and we will introduce the details of MLA and DeepSeekMoE in this section. For other tiny details (e.g., layer normalization and the activation function in FFNs), unless specifically stated, DeepSeek-V2 follows the settings of DeepSeek 67B (DeepSeek-AI, 2024).

æ€»ä½“è€Œè¨€ï¼ŒDeepSeek-V2 ä»é‡‡ç”¨ Transformer æ¶æ„ï¼ˆVaswani et al., 2017ï¼‰ï¼Œå…¶ä¸­æ¯ä¸ª Transformer å—ç”±ä¸€ä¸ªæ³¨æ„æ¨¡å—å’Œä¸€ä¸ªå‰é¦ˆç½‘ç»œ (FFN) ç»„æˆã€‚ç„¶è€Œï¼Œå¯¹äºæ³¨æ„æ¨¡å—å’Œ FFNï¼Œæˆ‘ä»¬éƒ½è®¾è®¡å¹¶é‡‡ç”¨äº†åˆ›æ–°æ¶æ„ã€‚å¯¹äºæ³¨æ„ï¼Œæˆ‘ä»¬è®¾è®¡äº† MLAï¼Œå®ƒåˆ©ç”¨ä½ç§©é”®å€¼è”åˆå‹ç¼©æ¥æ¶ˆé™¤æ¨ç†æ—¶é—´é”®å€¼ç¼“å­˜çš„ç“¶é¢ˆï¼Œä»è€Œæ”¯æŒé«˜æ•ˆæ¨ç†ã€‚å¯¹äº FFNï¼Œæˆ‘ä»¬é‡‡ç”¨ DeepSeekMoE æ¶æ„ï¼ˆDai et al., 2024ï¼‰ï¼Œè¿™æ˜¯ä¸€ç§é«˜æ€§èƒ½çš„ MoE æ¶æ„ï¼Œèƒ½å¤Ÿä»¥ç»æµçš„æˆæœ¬è®­ç»ƒå‡ºå¼ºå¤§çš„æ¨¡å‹ã€‚å›¾ 2 å±•ç¤ºäº† DeepSeek-V2 çš„æ¶æ„ï¼Œæˆ‘ä»¬å°†åœ¨æœ¬èŠ‚ä¸­ä»‹ç» MLA å’Œ DeepSeekMoE çš„ç»†èŠ‚ã€‚å¯¹äºå…¶ä»–å¾®å°çš„ç»†èŠ‚ï¼ˆä¾‹å¦‚ï¼Œå±‚è§„èŒƒåŒ–å’Œ FFN ä¸­çš„æ¿€æ´»å‡½æ•°ï¼‰ï¼Œé™¤éç‰¹åˆ«è¯´æ˜ï¼ŒDeepSeek-V2 éµå¾ª DeepSeek 67Bï¼ˆDeepSeek-AIï¼Œ2024ï¼‰çš„è®¾ç½®ã€‚

## 2.1 Multi-Head Latent Attention: Boosting Inference Efficiency

Conventional Transformer models usually adopts Multi-Head Attention (MHA) (Vaswani et al., 2017), but during generation, its heavy Key-Value (KV) cache will become the bottleneck that limit the inference efficiency. In order to reduce the KV cache, Multi-Query Attention (MQA) (Shazeer, 2019) and Grouped-Query Attention (GQA) (Ainslie et al., 2023) are proposed. They require a smaller magnitude of KV cache, but their performance does not match MHA (we provide the ablation of MHA, GQA and MQA in Appendix D.1). 

ä¼ ç»Ÿçš„Transformeræ¨¡å‹é€šå¸¸é‡‡ç”¨å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMHAï¼‰ï¼ˆVaswaniç­‰ï¼Œ2017ï¼‰ï¼Œä½†åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ï¼Œå…¶ç¹é‡çš„é”®å€¼ï¼ˆKVï¼‰ç¼“å­˜å°†æˆä¸ºé™åˆ¶æ¨ç†æ•ˆç‡çš„ç“¶é¢ˆã€‚ä¸ºäº†å‡å°‘KVç¼“å­˜ï¼Œæå‡ºäº†å¤šæŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMQAï¼‰ï¼ˆShazeerï¼Œ2019ï¼‰å’Œåˆ†ç»„æŸ¥è¯¢æ³¨æ„åŠ›æœºåˆ¶ï¼ˆGQAï¼‰ï¼ˆAinslieç­‰ï¼Œ2023ï¼‰ã€‚å®ƒä»¬éœ€è¦çš„KVç¼“å­˜é‡çº§è¾ƒå°ï¼Œä½†æ€§èƒ½ä¸å¦‚MHAï¼ˆæˆ‘ä»¬åœ¨é™„å½•D.1ä¸­æä¾›äº†MHAï¼ŒGQAå’ŒMQAçš„æ¶ˆèï¼‰ã€‚

For DeepSeek-V2, we design an innovative attention mechanism called Multi-head Latent Attention (MLA). Equipped with low-rank key-value joint compression, MLA achieves better performance than MHA, but requires a significantly smaller amount of KV cache. We introduce its architecture in the following, and also provide a comparison between MLA and MHA in Appendix D.2.

å¯¹äºDeepSeek-V2ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§åˆ›æ–°çš„æ³¨æ„åŠ›æœºåˆ¶ï¼Œç§°ä¸ºå¤šå¤´æ½œåœ¨æ³¨æ„åŠ›æœºåˆ¶ï¼ˆMLAï¼‰ã€‚MLAé…å¤‡äº†ä½ç§©é”®å€¼è”åˆå‹ç¼©ï¼Œæ€§èƒ½ä¼˜äºMHAï¼Œä½†éœ€è¦çš„KVç¼“å­˜é‡æ˜æ˜¾è¾ƒå°‘ã€‚æˆ‘ä»¬åœ¨ä¸‹é¢ä»‹ç»å®ƒçš„æ¶æ„ï¼Œå¹¶åœ¨é™„å½• D.2 ä¸­æä¾› MLA å’Œ MHA çš„æ¯”è¾ƒã€‚

  

### 2.1.1 Preliminaries: Standard Multi-Head Attention


We first introduce the standard MHA mechanism as background. Let ğ‘‘ be the embedding dimension, ğ‘›â„ be the number of attention heads, ğ‘‘â„ be the dimension per head, and hğ‘¡ âˆˆ Rğ‘‘ be the attention input of the ğ‘¡-th token at an attention layer. Standard MHA first produces qğ‘¡ , kğ‘¡ , vğ‘¡ âˆˆ Rğ‘‘â„ğ‘›â„ through three matrices ğ‘Šğ‘„ ,ğ‘Šğ¾ ,ğ‘Šğ‘‰ âˆˆ Rğ‘‘â„ğ‘›â„Ã—ğ‘‘ , respectively:

æˆ‘ä»¬é¦–å…ˆä»‹ç»æ ‡å‡†çš„ MHA æœºåˆ¶ä½œä¸ºèƒŒæ™¯ã€‚ä»¤ ğ‘‘ ä¸º embedding ç»´åº¦ï¼Œğ‘›â„ ä¸ºæ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼Œğ‘‘â„ ä¸ºæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œhğ‘¡ âˆˆ Rğ‘‘ ä¸ºæ³¨æ„åŠ›å±‚ä¸Šç¬¬ ğ‘¡ ä¸ª token çš„æ³¨æ„åŠ›è¾“å…¥ã€‚æ ‡å‡† MHA é¦–å…ˆé€šè¿‡ä¸‰ä¸ªçŸ©é˜µ ğ‘Šğ‘„ ,ğ‘Šğ¾ ,ğ‘Šğ‘‰ âˆˆ Rğ‘‘â„ğ‘›â„Ã—ğ‘‘ åˆ†åˆ«ç”Ÿæˆ qğ‘¡ , kğ‘¡ , vğ‘¡ âˆˆ Rğ‘‘â„ğ‘›â„ï¼š



Then, qğ‘¡ , kğ‘¡ , vğ‘¡ will be sliced into ğ‘›â„ heads for the multi-head attention computation
ç„¶åï¼Œqğ‘¡ã€kğ‘¡ã€vğ‘¡ å°†è¢«åˆ‡åˆ†ä¸º ğ‘›â„ ä¸ªå¤´ï¼Œä»¥è¿›è¡Œå¤šå¤´æ³¨æ„åŠ›è®¡ç®—



where qğ‘¡,ğ‘– , kğ‘¡,ğ‘– , vğ‘¡,ğ‘– âˆˆ Rğ‘‘â„ denote the query, key, and value of the ğ‘–-th attention head, respectively; ğ‘Šğ‘‚ âˆˆ Rğ‘‘Ã—ğ‘‘â„ğ‘›â„ denotes the output projection matrix. During inference, all keys and values need to be cached to accelerate inference, so MHA needs to cache 2ğ‘›â„ğ‘‘â„ğ‘™ elements for each token. In model deployment, this heavy KV cache is a large bottleneck that limits the maximum batch size and sequence length.
å…¶ä¸­ qğ‘¡,ğ‘– ã€kğ‘¡,ğ‘– ã€vğ‘¡,ğ‘– âˆˆ Rğ‘‘â„ åˆ†åˆ«è¡¨ç¤ºç¬¬ ğ‘– ä¸ªæ³¨æ„åŠ›å¤´çš„ queryã€key å’Œ valueï¼›ğ‘Šğ‘‚ âˆˆ Rğ‘‘Ã—ğ‘‘â„ğ‘›â„ è¡¨ç¤ºè¾“å‡ºæŠ•å½±çŸ©é˜µã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œéœ€è¦ç¼“å­˜æ‰€æœ‰ key å’Œ value ä»¥åŠ é€Ÿæ¨ç†ï¼Œå› æ­¤ MHA éœ€è¦ä¸ºæ¯ä¸ª token ç¼“å­˜ 2ğ‘›â„ğ‘‘â„ğ‘™ ä¸ªå…ƒç´ ã€‚åœ¨æ¨¡å‹éƒ¨ç½²ä¸­ï¼Œè¿™ç§ç¹é‡çš„ KV ç¼“å­˜æ˜¯é™åˆ¶æœ€å¤§æ‰¹æ¬¡å¤§å°å’Œåºåˆ—é•¿åº¦çš„ä¸€å¤§ç“¶é¢ˆã€‚

### 2.1.2 Low-Rank Key-Value Joint Compression

The core of MLA is the low-rank joint compression for keys and values to reduce KV cache:
MLAçš„æ ¸å¿ƒæ˜¯å¯¹keyå’Œvalueè¿›è¡Œä½ç§©è”åˆå‹ç¼©ï¼Œä»¥å‡å°‘KVç¼“å­˜ï¼š




where c ğ¾ğ‘‰ ğ‘¡ âˆˆ Rğ‘‘ğ‘ is the compressed latent vector for keys and values; ğ‘‘ğ‘(â‰ª ğ‘‘â„ğ‘›â„) denotes the KV compression dimension; ğ‘Šğ·ğ¾ğ‘‰ âˆˆ Rğ‘‘ğ‘Ã—ğ‘‘ is the down-projection matrix; and ğ‘Šğ‘ˆğ¾ ,ğ‘Šğ‘ˆğ‘‰ âˆˆ Rğ‘‘â„ğ‘›â„Ã—ğ‘‘ğ‘ are the up-projection matrices for keys and values, respectively. During inference, MLA only needs to cache c ğ¾ğ‘‰ ğ‘¡ , so its KV cache has only ğ‘‘ğ‘ ğ‘™ elements, where ğ‘™ denotes the number of layers. In addition, during inference, since ğ‘Šğ‘ˆğ¾ can be absorbed into ğ‘Šğ‘„ , and ğ‘Šğ‘ˆğ‘‰ can be absorbed into ğ‘Šğ‘‚ , we even do not need to compute keys and values out for attention. Figure 3 intuitively illustrates how the KV joint compression in MLA reduces the KV cache.
å…¶ä¸­ c ğ¾ğ‘‰ ğ‘¡ âˆˆ Rğ‘‘ğ‘ æ˜¯é”®å’Œå€¼çš„å‹ç¼©æ½œåœ¨å‘é‡ï¼›ğ‘‘ğ‘(â‰ª ğ‘‘â„ğ‘›â„) è¡¨ç¤º KV å‹ç¼©ç»´åº¦ï¼›ğ‘Šğ·ğ¾ğ‘‰ âˆˆ Rğ‘‘ğ‘Ã—ğ‘‘ æ˜¯ä¸‹æŠ•å½±çŸ©é˜µï¼›ğ‘Šğ‘ˆğ¾ ,ğ‘Šğ‘ˆğ‘‰ âˆˆ Rğ‘‘â„ğ‘›â„Ã—ğ‘‘ğ‘ åˆ†åˆ«æ˜¯é”®å’Œå€¼çš„ä¸ŠæŠ•å½±çŸ©é˜µã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼ŒMLA åªéœ€è¦ç¼“å­˜ c ä¸ª ğ¾ğ‘‰ ğ‘¡ ï¼Œå› æ­¤å…¶ KV ç¼“å­˜åªæœ‰ ğ‘‘ğ‘ ğ‘™ ä¸ªå…ƒç´ ï¼Œå…¶ä¸­ ğ‘™ è¡¨ç¤ºå±‚æ•°ã€‚æ­¤å¤–ï¼Œåœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œç”±äº ğ‘Šğ‘ˆğ¾ å¯ä»¥è¢«å¸æ”¶åˆ° ğ‘Šğ‘„ ä¸­ï¼Œ ğ‘Šğ‘ˆğ‘‰ å¯ä»¥è¢«å¸æ”¶åˆ° ğ‘Šğ‘‚ ä¸­ï¼Œæˆ‘ä»¬ç”šè‡³ä¸éœ€è¦è®¡ç®—å‡ºç”¨äºæ³¨æ„çš„é”®å’Œå€¼ã€‚å›¾ 3 ç›´è§‚åœ°è¯´æ˜äº† MLA ä¸­çš„ KV è”åˆå‹ç¼©å¦‚ä½•å‡å°‘ KV ç¼“å­˜ã€‚
Moreover, in order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache:
æ­¤å¤–ï¼Œä¸ºäº†å‡å°‘è®­ç»ƒæœŸé—´çš„æ¿€æ´»å†…å­˜ï¼Œæˆ‘ä»¬è¿˜å¯¹æŸ¥è¯¢æ‰§è¡Œä½ç§©å‹ç¼©ï¼Œå³ä½¿å®ƒä¸èƒ½å‡å°‘ KV ç¼“å­˜ï¼š


where c ğ‘„ ğ‘¡ âˆˆ Rğ‘‘ â€² ğ‘ is the compressed latent vector for queries; ğ‘‘ â€² ğ‘ (â‰ª ğ‘‘â„ğ‘›â„) denotes the query compression dimension; and ğ‘Šğ·ğ‘„ âˆˆ Rğ‘‘ â€² ğ‘Ã—ğ‘‘ ,ğ‘Šğ‘ˆğ‘„ âˆˆ Rğ‘‘â„ğ‘›â„Ã—ğ‘‘ â€² ğ‘ are the down-projection and upprojection matrices for queries, respectively.

å…¶ä¸­ c ğ‘„ ğ‘¡ âˆˆ Rğ‘‘ â€² ğ‘ æ˜¯æŸ¥è¯¢çš„å‹ç¼©æ½œåœ¨å‘é‡ï¼›ğ‘‘ â€² ğ‘ (â‰ª ğ‘‘â„ğ‘›â„) è¡¨ç¤ºæŸ¥è¯¢å‹ç¼©ç»´åº¦ï¼›ğ‘Šğ·ğ‘„ âˆˆ Rğ‘‘ â€² ğ‘Ã—ğ‘‘ ,ğ‘Šğ‘ˆğ‘„ âˆˆ Rğ‘‘â„ğ‘›â„Ã—ğ‘‘ â€² ğ‘ åˆ†åˆ«æ˜¯æŸ¥è¯¢çš„ä¸‹æŠ•å½±å’Œä¸ŠæŠ•å½±çŸ©é˜µã€‚

### 2.1.3 Decoupled Rotary Position Embedding


Following DeepSeek 67B (DeepSeek-AI, 2024), we intend to use the Rotary Position Embedding (RoPE) (Su et al., 2024) for DeepSeek-V2. However, RoPE is incompatible with low-rank KV compression. To be specific, RoPE is position-sensitive for both keys and queries. If we apply RoPE for the keys k ğ¶ ğ‘¡ , ğ‘Šğ‘ˆğ¾ in Equation 10 will be coupled with a position-sensitive RoPE matrix. In this way, ğ‘Šğ‘ˆğ¾ cannot be absorbed into ğ‘Šğ‘„ any more during inference, since a RoPE matrix related to the currently generating token will lie between ğ‘Šğ‘„ and ğ‘Šğ‘ˆğ¾ and matrix multiplication does not obey a commutative law. As a result, we must recompute the keys for all the prefix tokens during inference, which will significantly hinder the inference efficiency.

ç»§ DeepSeek 67Bï¼ˆDeepSeek-AIï¼Œ2024 å¹´ï¼‰ä¹‹åï¼Œæˆ‘ä»¬æ‰“ç®—åœ¨ DeepSeek-V2 ä¸­ä½¿ç”¨æ—‹è½¬ä½ç½®åµŒå…¥ (RoPE)ï¼ˆSu ç­‰äººï¼Œ2024 å¹´ï¼‰ã€‚ä½†æ˜¯ï¼ŒRoPE ä¸ä½ç§© KV å‹ç¼©ä¸å…¼å®¹ã€‚å…·ä½“æ¥è¯´ï¼ŒRoPE å¯¹é”®å’ŒæŸ¥è¯¢éƒ½æ˜¯ä½ç½®æ•æ„Ÿçš„ã€‚å¦‚æœæˆ‘ä»¬å°† RoPE ç”¨äºé”® k ğ¶ ğ‘¡ ï¼Œåˆ™ç­‰å¼ 10 ä¸­çš„ ğ‘Šğ‘ˆğ¾ å°†ä¸ä½ç½®æ•æ„Ÿçš„ RoPE çŸ©é˜µè€¦åˆã€‚è¿™æ ·ï¼Œğ‘Šğ‘ˆğ¾ åœ¨æ¨ç†è¿‡ç¨‹ä¸­å°±æ— æ³•å†è¢«å¸æ”¶åˆ° ğ‘Šğ‘„ ä¸­ï¼Œå› ä¸ºä¸å½“å‰ç”Ÿæˆçš„ token ç›¸å…³çš„ RoPE çŸ©é˜µå°†ä½äº ğ‘Šğ‘„ å’Œ ğ‘Šğ‘ˆğ¾ ä¹‹é—´ï¼Œå¹¶ä¸”çŸ©é˜µä¹˜æ³•ä¸éµå¾ªäº¤æ¢å¾‹ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¿…é¡»åœ¨æ¨ç†è¿‡ç¨‹ä¸­é‡æ–°è®¡ç®—æ‰€æœ‰å‰ç¼€æ ‡è®°çš„é”®ï¼Œè¿™å°†ä¸¥é‡é˜»ç¢æ¨ç†æ•ˆç‡ã€‚


As a solution, we propose the decoupled RoPE strategy that uses additional multi-head queries q ğ‘… ğ‘¡,ğ‘– âˆˆ Rğ‘‘ ğ‘… â„ and a shared key k ğ‘… ğ‘¡ âˆˆ Rğ‘‘ ğ‘… â„ to carry RoPE, where ğ‘‘ ğ‘… â„ denotes the per-head dimension of the decoupled queries and key. Equipped with the decoupled RoPE strategy, MLA performs the following computation:

ä½œä¸ºè§£å†³æ–¹æ¡ˆï¼Œæˆ‘ä»¬æå‡ºäº†è§£è€¦ RoPE ç­–ç•¥ï¼Œè¯¥ç­–ç•¥ä½¿ç”¨é¢å¤–çš„å¤šå¤´æŸ¥è¯¢ q ğ‘… ğ‘¡,ğ‘– âˆˆ Rğ‘‘ ğ‘… â„ å’Œå…±äº«å¯†é’¥ k ğ‘… ğ‘¡ âˆˆ Rğ‘‘ ğ‘… â„ æ¥æ‰¿è½½ RoPEï¼Œå…¶ä¸­ ğ‘‘ ğ‘… â„ è¡¨ç¤ºè§£è€¦æŸ¥è¯¢å’Œå¯†é’¥çš„æ¯ä¸ªå¤´ç»´åº¦ã€‚é…å¤‡è§£è€¦ RoPE ç­–ç•¥åï¼ŒMLA å¯æ‰§è¡Œä»¥ä¸‹è®¡ç®—ï¼š




where ğ‘Šğ‘„ğ‘… âˆˆ Rğ‘‘ ğ‘… â„ ğ‘›â„Ã—ğ‘‘ â€² ğ‘ and ğ‘Šğ¾ğ‘… âˆˆ Rğ‘‘ ğ‘… â„ Ã—ğ‘‘ are matrices to produce the decouples queries and key, respectively; RoPE(Â·) denotes the operation that applies RoPE matrices; and [Â·; Â·] denotes the concatenation operation. During inference, the decoupled key should also be cached. Therefore, DeepSeek-V2 requires a total KV cache containing (ğ‘‘ğ‘ + ğ‘‘ ğ‘… â„ )ğ‘™ elements. 

å…¶ä¸­ ğ‘Šğ‘„ğ‘… âˆˆ Rğ‘‘ ğ‘… â„ ğ‘›â„Ã—ğ‘‘ â€² ğ‘ å’Œ ğ‘Šğ¾ğ‘… âˆˆ Rğ‘‘ ğ‘… â„ Ã—ğ‘‘ åˆ†åˆ«æ˜¯ç”Ÿæˆè§£è€¦æŸ¥è¯¢å’Œé”®çš„çŸ©é˜µï¼›RoPE(Â·) è¡¨ç¤ºåº”ç”¨ RoPE çŸ©é˜µçš„æ“ä½œï¼›[Â·; Â·] è¡¨ç¤ºè¿æ¥æ“ä½œã€‚åœ¨æ¨ç†è¿‡ç¨‹ä¸­ï¼Œè§£è€¦çš„é”®ä¹Ÿåº”è¯¥è¢«ç¼“å­˜ã€‚å› æ­¤ï¼ŒDeepSeek-V2 éœ€è¦ä¸€ä¸ªåŒ…å« (ğ‘‘ğ‘ + ğ‘‘ ğ‘… â„ )ğ‘™ å…ƒç´ çš„æ€» KV ç¼“å­˜ã€‚

In order to demonstrate the complete computation process of MLA, we also organize and provide its full formulas in Appendix C.

ä¸ºäº†å±•ç¤º MLA çš„å®Œæ•´è®¡ç®—è¿‡ç¨‹ï¼Œæˆ‘ä»¬è¿˜åœ¨é™„å½• C ä¸­æ•´ç†å¹¶æä¾›äº†å…¶å®Œæ•´å…¬å¼ã€‚

###2.1.4. Comparison of Key-Value Cache
  
We demonstrate a comparison of the KV cache per token among different attention mechanisms in Table 1. MLA requires only a small amount of KV cache, equal to GQA with only 2.25 groups, but can achieve stronger performance than MHA.

æˆ‘ä»¬åœ¨è¡¨ 1 ä¸­å±•ç¤ºäº†ä¸åŒæ³¨æ„åŠ›æœºåˆ¶ä¸­æ¯ä¸ª token çš„ KV ç¼“å­˜çš„æ¯”è¾ƒã€‚MLA åªéœ€è¦å°‘é‡çš„ KV ç¼“å­˜ï¼Œç›¸å½“äºåªæœ‰ 2.25 ä¸ªç»„çš„ GQAï¼Œä½†å¯ä»¥è·å¾—æ¯” MHA æ›´å¼ºçš„æ€§èƒ½ã€‚


Table1 ä¸åŒæ³¨æ„åŠ›æœºåˆ¶æ¯ä¸ª token çš„ KV ç¼“å­˜å¯¹æ¯”ã€‚ğ‘›â„ è¡¨ç¤ºæ³¨æ„åŠ›å¤´æ•°é‡ï¼Œğ‘‘â„ è¡¨ç¤ºæ¯ä¸ªæ³¨æ„åŠ›å¤´çš„ç»´åº¦ï¼Œğ‘™ è¡¨ç¤ºå±‚æ•°ï¼Œğ‘›ğ‘” è¡¨ç¤º GQA ä¸­çš„ç»„æ•°ï¼Œğ‘‘ğ‘ å’Œ ğ‘‘ ğ‘… â„ åˆ†åˆ«è¡¨ç¤º MLA ä¸­è§£è€¦æŸ¥è¯¢å’Œé”®çš„ KV å‹ç¼©ç»´åº¦å’Œæ¯ä¸ªå¤´çš„ç»´åº¦ã€‚KV ç¼“å­˜é‡ä»¥å…ƒç´ æ•°é‡è¡¡é‡ï¼Œä¸å­˜å‚¨ç²¾åº¦æ— å…³ã€‚å¯¹äº DeepSeek-V2ï¼Œğ‘‘ğ‘ è®¾ç½®ä¸º 4ğ‘‘â„ï¼Œğ‘‘ ğ‘… â„ è®¾ç½®ä¸º ğ‘‘â„ 2 ã€‚å› æ­¤ï¼Œå®ƒçš„ KV ç¼“å­˜ä¸åªæœ‰ 2.25 ä¸ªç»„çš„ GQA ç›¸åŒï¼Œä½†å…¶æ€§èƒ½å¼ºäº MHA
